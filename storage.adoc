## Lab: Using Persistent Storage

### Background

One of the concepts of being Cloud-Native is to be stateless by externalizing state.
But this state still needs to be stored somewhere and working with a Container Platform you would prefer this state to be containerized too.
OpenShift makes it easy to host for example a MySQL database with persistent storage mounted to it.
Just like pods can move fluently between nodes in a cluster, Network Attached Storage in the form of a Persistent Volume Claim (PVC) can move just as fluently together with the pod.

A https://{{DOCS_URL}}/latest/architecture/additional_concepts/storage.html[Persistent Volume Claim (PVC)] creates an abstraction between the consumer claiming storage space for his application, and the underlaying infrastructure providing the storage.
This way the consumer doesn't need to know the technical details and will instantly get a storage volume assigned based on his quota and availability from the pool.

The cluster-operator can integrate OpenShift with his existing SAN/NAS-infrastructure (like FibreChannel, NFS, iSCSI, Cinder, AzureDisk, ...), or can run our unique https://docs.openshift.com/container-platform/3.5/install_config/persistent_storage/persistent_storage_glusterfs.html[Container Native Storage (CNS)] that will run a Gluster Storage cluster natively in containers, thereby creating a Hyper-converged platform providing Storage, Networking, Compute & Security.


#### Exercise: Create Claim

Our MongoDB currently is ephemeral, we can store data in it, but as soon as we kill & restart the container our state is gone and MongoDB starts with an empty database.
Let's add a Persistent Volume to it where we can persist our data onto. All we need to do is create a Persistent Volume Claim (PVC) where we specify what type and how much storage we want.

Go to menu "Storage", then click the button "Create Storage".
Fill in the following details:

* Name          = mongodb-data
* Access Mode   = Single User (RWO)  (this will only permit one pod to mount this volume)
* Size          = 2 Gi

image::storage-1.png[Storage Overview]
image::storage-2.png[Create Storage]

You should now see your Claim automatically bound to a Persistent Volume.

#### Exercise: Attach Claim

Now attach our Persistent Volume to our application (DeploymentConfig)

```
oc set volumes dc/mongodb --add --claim-name=mongodb-data -m /var/lib/mongodb/data --name mongodb-data --overwrite --type persistentVolumeClaim
```

This configuration change to our DeploymentConfig has triggered a re-deploy of our application.

#### Exercise: Validate Storage
We should now see our MongoDB startup with persistent storage. 
We can validate this by looking at the mount points inside the container:

```
oc rsh dc/mongodb mount |grep /var/lib/mongodb/data
```

if we kill and restart the container the data was persisted before and is still available.

```
oc rsh dc/mongodb touch /var/lib/mongodb/data/{{USER_NAME}}-was-here
oc rsh dc/mongodb ls /var/lib/mongodb/data
oc delete pod -l app=mongodb
oc rsh dc/mongodb ls /var/lib/mongodb/data
```

#### Exercise: Where has my data gone?

As we have made the database persistent, we can again visit the `nationalparks` web
service to query for data:

[source]
----
http://nationalparks-{{EXPLORE_PROJECT_NAME}}{{USER_SUFFIX}}.{{ROUTER_ADDRESS}}/ws/data/all
----

And the result?

[source]
----
[]
----

Where's the data? Think about the process you went through. You deployed the
the database, loaded the data, and changed the database location into a Persistent Volume. 
The data that you loaded is gone. 

Let's load the data back again. As we described before, the application provides an endpoint to do just that:

[source]
----
http://nationalparks-{{EXPLORE_PROJECT_NAME}}{{USER_SUFFIX}}.{{ROUTER_ADDRESS}}/ws/data/load
----

And the result?

[source]
----
Items inserted in database: 2740
----

If you then go back to `/ws/data/all` you will see tons of JSON data now.
That's great. Our parks map will work again!

[source]
----
http://parksmap-{{EXPLORE_PROJECT_NAME}}{{USER_SUFFIX}}.{{ROUTER_ADDRESS}}
----
